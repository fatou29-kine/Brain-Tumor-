{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatou29-kine/Brain-Tumor-/blob/main/MLSS-DNN-DAY2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J1XOB6S3WU5"
      },
      "source": [
        "# Parallel and distributed Deep Learning\n",
        "\n",
        "## Author: Marieme Ngom, Argonne National Laboratory\n",
        "(combining and adapting materials/discussion evolved over time by Huihuo Zheng, Bethany Lusch, Asad Khan, Prasanna Balaprakash, Taylor Childers, Corey Adams, Kyle Felker, Varuni Sastry, Sam Foreman, Archit Vasan, Carlo Graziani, Tanwi Mallick, and Venkat Vishwanath)\n",
        "## Outline\n",
        "1. Day 1\n",
        "    - Evolution of computig systems\n",
        "    - Parallel computing\n",
        "    - Introduction to Deep Learning\n",
        "    - ***Parallel computing in AI***\n",
        "\n",
        "\n",
        "2. ***Day 2***\n",
        "    - ***Parallel computing in AI***\n",
        "    - Brief Introduction to LLMs\n",
        "    - Hands-on LLM training\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fi8tbPRp3btk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-UYlGVgd3c5g",
        "outputId": "eb566eb8-55da-4f89-c112-6cba485e8e89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIhqt4cH3WVA"
      },
      "source": [
        "# Parallel Computing in AI\n",
        "### Recap Single GPU\n",
        "![x0](https://github.com/mngom2/DNNMLSS/blob/main/images/mermaid-figure-1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSttCdqU3WVB"
      },
      "source": [
        "Distributed training is the process of training models across multiple GPUs or other accelerators, with the goal of speeding up the training process and enabling the training of larger models on larger datasets.\n",
        "\n",
        "There are two ways of parallelization in distributed training.\n",
        "* ***Data parallelism***:\n",
        "    * Each worker (GPU) has a complete set of model\n",
        "    * different workers work on different subsets of data.\n",
        "* *Model parallelism*\n",
        "    * The model is split into different parts and stored on different workers\n",
        "    * Different workers work on computation involved in different parts of the model\n",
        "![PI](https://github.com/mngom2/DNNMLSS/blob/main/images/parallel_computing.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ1FY1uS3WVC"
      },
      "source": [
        "## Scaling goal:\n",
        "1. Minimize cost i.e. amount of time spent training\n",
        "2. Maximize performance i.e model quality metrics, throughput/efficiency metrics (images/seconds, GPU/CPU utilization percentages, flops efficiency)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj4JB8-S3WVD"
      },
      "source": [
        "## Training on multiple GPUs: Data Parallelism\n",
        "### Nomenclature:\n",
        "- N = number of GPUs = WORLD_SIZE\n",
        "- Each GPU is assigned a rank from 0 to WORLD_SIZE-1\n",
        "- A worker = a GPU here\n",
        "![mgpus](https://github.com/mngom2/DNNMLSS/blob/main/images/mermaid-figure-15.png?raw=1)\n",
        "*Each GPU receives unique data at each step*\n",
        "### Data Parallel: Forward Pass\n",
        "![forward](https://github.com/mngom2/DNNMLSS/blob/main/images/mermaid-figure-14.png?raw=1)\n",
        "*Average gradients across all GPUs*\n",
        "### Data Parallel: Backward Pass\n",
        "![backward](https://github.com/mngom2/DNNMLSS/blob/main/images/mermaid-figure-13.png?raw=1)\n",
        "*Send global updates back to each GPU*\n",
        "### Data Parallel: Full Setup\n",
        "![full](https://github.com/mngom2/DNNMLSS/blob/main/images/mermaid-figure-12.png?raw=1)\n",
        "*See: [PyTorch / Distributed Data Parallel](https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html)*\n",
        "### Data Parallel: Training\n",
        "- Each GPU:\n",
        "    - has identical copy of model\n",
        "    - works on a unique subset of data\n",
        "- Easy to get started with\n",
        "    - [saforeman2/ezpz](https://github.com/saforem2/ezpz)\n",
        "    - [PyTorch/DDP](https://docs.pytorch.org/docs/stable/notes/ddp.html)\n",
        "    - [HF/Accelerate](https://huggingface.co/docs/transformers/accelerate)\n",
        "    - [Microsoft/DeepSpeed](https://www.deepspeed.ai)\n",
        "- Requires ***global*** communication\n",
        "    - every rank must participate (collective communication)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi3NLiY13WVE"
      },
      "source": [
        "## Communication\n",
        "- Need mechanism(s) for communicating across GPUs:\n",
        "    - [mpi4py](https://mpi4py.readthedocs.io/en/stable/tutorial.html)\n",
        "    - [torch.distributed](https://docs.pytorch.org/docs/stable/distributed.html)\n",
        "- Collective communication:\n",
        "    - [Nvidia Collective Communications Library (NCCL)](https://developer.nvidia.com/nccl)\n",
        "    - [Intel oneAPI Collective Communications](https://www.intel.com/content/www/us/en/developer/tools/oneapi/oneccl.html#gs.n9y302)\n",
        "***Timeouts*** Collective operations have to be called for each rank to form a complete collective operation.\n",
        "Failure to do so will result in other ranks waiting indefinitely\n",
        "### AllReduce\n",
        "![allreduce](https://github.com/mngom2/DNNMLSS/blob/main/images/mermaid-figure-11.png?raw=1)\n",
        "\n",
        "### Broadcast\n",
        "![broadcast](https://github.com/mngom2/DNNMLSS/blob/main/images/mermaid-figure-9.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDAiJ0Jy3WVF"
      },
      "source": [
        "## Dealing with Data\n",
        "- At each training step, we want to ensure that each worker receives unique data\n",
        "- This can be done in one of two ways:\n",
        "    1. Manually partition data (ahead of time)\n",
        "        - Assign unique subsets to each worker\n",
        "        - Each worker can only see their local portion of the data\n",
        "        - Most common approach\n",
        "    2. From each worker, randomly select a mini-batch\n",
        "        - Each worker can see the full dataset\n",
        "        - ⚠️ When randomly selecting, it is important that each worker uses different seeds to ensure they receive unique data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEQ7vLRy3WVH"
      },
      "source": [
        "## Broadcast Initial State\n",
        "- At the start of training (or when loading from a checkpoint), we want all of our workers to be initialized consistently\n",
        "    - Broadcast the model and optimizer states from rank() == 0 worker\n",
        "![bcast](https://github.com/mngom2/DNNMLSS/blob/main/images/mermaid-figure-6.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe9ghpYC3WVI"
      },
      "source": [
        "## Why distributed training?\n",
        "- N workers each processing unique batch (micro batch size) of data:\n",
        "    - (micro_batch_size = 1)× $N_{GPUs}$ → ***global_batch_size = N***\n",
        "- Improved gradient estimators\n",
        "    - Smooth loss landscape\n",
        "    - Less iterations needed for same number of epochs\n",
        "        - common to scale learning rate lr *= sqrt(N)\n",
        "        \n",
        "![speedup](https://github.com/mngom2/DNNMLSS/blob/main/images/speedup.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWLqwkP23WVJ"
      },
      "source": [
        "## Going Beyond Data Parallelism\n",
        "- Useful when model fits on single GPU:\n",
        "    - ultimately limited by GPU memory\n",
        "    - model performance limited by size\n",
        "- ⚠️ When model does not fit on a single GPU:\n",
        "    - Offloading (can only get you so far…):\n",
        "        - DeepSpeed + ZeRO\n",
        "        - PyTorch + FSDP\n",
        "- Otherwise, resort to [model parallelism strategies](https://samforeman.me/talks/ai-for-science-2024/slides#/additional-parallelism-strategies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W90iAiCD3WVJ"
      },
      "source": [
        "# Going beyond Data Parallelism:  DeepSpeed + ZeRO\n",
        "- Depending on the ZeRO stage (1, 2, 3), we can offload\n",
        "    1. ***Stage 1***: optimizer states (P_{os})\n",
        "    2. ***Stage 2***: optimizer states+gradients (P_{os+g})\n",
        "    2. ***Stage 3***: optimizer states+gradients+model params (P_{os+g+p})\n",
        "\n",
        "![zero](https://github.com/mngom2/DNNMLSS/blob/main/images/zero.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvZQh0cB3WVK"
      },
      "source": [
        "# Model parallel training: example\n",
        "Want to compute $y = \\sum_i x_iW_i = x_0W_0 + x_1W_1 + x_2W_2$ where each GPU only has only its portion of the full weights as shown below\n",
        "1. Compute $y_0=x0W_0$ -> **GPU1**\n",
        "2. Compute $y_1=y_0 +x_1W_1$ -> **GPU2**\n",
        "3. Compute $y_2=y_1 + x_2W_2$\n",
        "\n",
        "![modelpar](https://github.com/mngom2/DNNMLSS/blob/main/images/mermaid-figure-2.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkQZUgnf3WVL"
      },
      "source": [
        "# Deciding on a parallelism strategy\n",
        "![onedec](https://github.com/mngom2/DNNMLSS/blob/main/images/onegpudec.png?raw=1)\n",
        "![multgpu](https://github.com/mngom2/DNNMLSS/blob/main/images/onenodemulgpu.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dspfa9t43WVL"
      },
      "source": [
        "![AIcompute](https://github.com/mngom2/DNNMLSS/blob/main/images/ai-and-compute-all-2.png.webp?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWCfYc1F3WVM"
      },
      "source": [
        "Sophia: 192 GPUs (8/node), 3.9 Petaflops ($10^15$)/s\n",
        "![sophia](https://github.com/mngom2/DNNMLSS/blob/main/images/sophia.jpeg?raw=1)\n",
        "Polaris: 2240 GPUs (4/node), 78 Teraflops ($10^12$)/s\n",
        "![polaris](https://github.com/mngom2/DNNMLSS/blob/main/images/polaris.jpeg?raw=1)\n",
        "Aurora: 63,744 GPUs (6/node), exascale computer ($10^18$ calculations per second)\n",
        "![aurora](https://github.com/mngom2/DNNMLSS/blob/main/images/aurora.jpeg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR4NfbF_3WVM"
      },
      "source": [
        "# Brief introduction to LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irMSnK893WVN"
      },
      "source": [
        "## Training LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqajpYAZ3WVN"
      },
      "source": [
        "## Life-cycle of a LLM\n",
        "1. Data collection + preprocessing\n",
        "2. ***Pre-training***\n",
        "    - Architecture decisions, model size, etc.\n",
        "3. Supervised Fine-Tuning\n",
        "    - Instruction Tuning\n",
        "    - Alignment\n",
        "4. Deploy (+ monitor, re-evaluate, etc.)\n",
        "\n",
        "![gptcycle](https://github.com/mngom2/DNNMLSS/blob/main/images/gpt3-training-step-back-prop.gif?raw=1)\n",
        "*Source:Figure from [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxSINv3P3WVO"
      },
      "source": [
        "## Life-cycle of a LLM\n",
        "1. Data collection + preprocessing\n",
        "2. Pre-training\n",
        "    - Architecture decisions, model size, etc.\n",
        "3. ***Supervised Fine-Tuning***\n",
        "    - Instruction Tuning\n",
        "    - Alignment\n",
        "4. Deploy (+ monitor, re-evaluate, etc.)\n",
        "\n",
        "![gptcycle2](https://github.com/mngom2/DNNMLSS/blob/main/images/gpt3-fine-tuning.gif?raw=1)\n",
        "*Source:Figure from [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F_l_eEm3WVO"
      },
      "source": [
        "## Forward pass\n",
        "![fwdpass](https://github.com/mngom2/DNNMLSS/blob/main/images/hf_assisted_generation.mov?raw=1)\n",
        "*Source: [Generation with LLMs](https://huggingface.co/docs/transformers/main/en/llm_tutorial)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYpEVM043WVP"
      },
      "source": [
        "## Generating text\n",
        "![fwdpass](https://github.com/mngom2/DNNMLSS/blob/main/images/hf_assisted_generation2.mov?raw=1)\n",
        "*Source: [Generation with LLMs](https://huggingface.co/docs/transformers/main/en/llm_tutorial)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML3pMQ3q3WVP"
      },
      "source": [
        "# Hands-on LLM Training\n",
        "\n",
        "\n",
        "***Good practice*** (not needed here): Create and activate a conda (or virtual) environment\n",
        "```conda create -n env_mlss_dnn python=3.9```\n",
        "then on jupyter do new ->terminal\n",
        "\n",
        "```\n",
        " conda activate env_mlss_dnn\n",
        " pip install ipykernel\n",
        " python -m ipykernel install --user --name env_mlss_dnn\n",
        "```\n",
        "\n",
        "then go back to your .ipynb file, change kernel to env_mlss_dnn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SNPvatLZ3WVQ",
        "outputId": "64343ae6-1e23-4577-f357-4e2b6bdb3c81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 686, done.\u001b[K\n",
            "remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n",
            "Receiving objects: 100% (686/686), 954.04 KiB | 12.08 MiB/s, done.\n",
            "Resolving deltas: 100% (387/387), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6JBYeubR3WVS",
        "outputId": "24ef3675-43ba-4066-ab33-1779de661fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/nanoGPT'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "%pwd\n",
        "\n",
        "%cd nanoGPT\n",
        "\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hfup4AwA3WVT",
        "outputId": "a75ed0c2-61f4-49a6-f50c-b96a0cbf917d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.30.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyWqisAL3WVU"
      },
      "source": [
        "| Dataset               | Tokens (≈)         | Disk size / notes                                 |\n",
        "| --------------------- | ------------------ | ------------------------------------------------- |\n",
        "| **openwebtext**       | 9 B tokens total   | 9 B train (≈17GB) / 4M val (≈8.5MB)                |\n",
        "| **shakespeare (tiny)** | ≈ 330K tokens total | 301,966 train / 36,059 val                   |\n",
        "| **shakespeare\\_char** | 1,115,394 chars    | 1,003,854 train / 111,540 val (character‐level)   |\n",
        "\n",
        "\n",
        "| model | params | train loss | val loss |\n",
        "| ------| ------ | ---------- | -------- |\n",
        "| gpt2 | 124M         | 3.11  | 3.12     |\n",
        "| gpt2-medium | 350M  | 2.85  | 2.84     |\n",
        "| gpt2-large | 774M   | 2.66  | 2.67     |\n",
        "| gpt2-xl | 1558M     | 2.56  | 2.54     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tUm-x8Ci3WVU",
        "outputId": "6d9e002b-ec68-4cc3-cec1-b390277976ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ],
      "source": [
        "!python3 data/shakespeare_char/prepare.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OIPGpddm3WVX",
        "outputId": "b6def265-0815-4fee-d760-a2622d94b3e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: compile = False\n",
            "Overriding: eval_iters = 20\n",
            "Overriding: log_interval = 1\n",
            "Overriding: block_size = 64\n",
            "Overriding: batch_size = 16\n",
            "Overriding: n_layer = 4\n",
            "Overriding: n_head = 4\n",
            "Overriding: n_embd = 128\n",
            "Overriding: max_iters = 2000\n",
            "Overriding: lr_decay_iters = 2000\n",
            "Overriding: dropout = 0.0\n",
            "tokens per iteration will be: 1,024\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 0.80M\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 193, in <module>\n",
            "    model.to(device)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1343, in to\n",
            "    return self._apply(convert)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
            "    param_applied = fn(param)\n",
            "                    ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1329, in convert\n",
            "    return t.to(\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\", line 319, in _lazy_init\n",
            "    torch._C._cuda_init()\n",
            "RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"
          ]
        }
      ],
      "source": [
        "!python3 train.py config/train_shakespeare_char.py --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=16 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YCC6gbZn3WVY",
        "outputId": "400824d0-7f52-4f33-8ba4-070ab9dfca94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-ayopwtkc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-ayopwtkc\n",
            "  Resolved https://github.com/openai/whisper.git to commit dd985ac4b90cafeef8712f2998d62c59c3e62d22\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803707 sha256=eb9b6226da8a3d1efb23d40dc8d0e1fcbcef421d1f29b0b4efdd85d0ed69f7ef\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qsmrqsz0/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20240930\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XwJyaFcs3WVZ"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eONuwXXS3WVZ",
        "outputId": "5b63e4bd-a68a-4e1f-815e-83dc90913b4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/sample.py\", line 38, in <module>\n",
            "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1425, in load\n",
            "    with _open_file_like(f, \"rb\") as opened_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 751, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 732, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "                     ^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'out-shakespeare-char/ckpt.pt'\n"
          ]
        }
      ],
      "source": [
        "!python3 sample.py --out_dir=out-shakespeare-char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CdvUF9Mi3WVa",
        "outputId": "fad96ae0-cd2b-48e6-ba66-50def8d52409",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 193, in <module>\n",
            "    model.to(device)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1343, in to\n",
            "    return self._apply(convert)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 903, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
            "    param_applied = fn(param)\n",
            "                    ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1329, in convert\n",
            "    return t.to(\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\", line 319, in _lazy_init\n",
            "    torch._C._cuda_init()\n",
            "RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n"
          ]
        }
      ],
      "source": [
        "!python3 train.py config/train_shakespeare_char.py #training longer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_CONieHz3WVb",
        "outputId": "06c7ac27-2328-42ee-8668-efca2598129f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/sample.py\", line 38, in <module>\n",
            "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1425, in load\n",
            "    with _open_file_like(f, \"rb\") as opened_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 751, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 732, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "                     ^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'out-shakespeare-char/ckpt.pt'\n"
          ]
        }
      ],
      "source": [
        "!python3 sample.py --out_dir=out-shakespeare-char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nyt9lOru3WVc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KjnZD1md3WVd"
      },
      "outputs": [],
      "source": [
        "!export NCCL_DEBUG=INFO\n",
        "!export NCCL_DEBUG_SUBSYS=ALL\n",
        "!export NCCL_DEBUG_FILE=nccl_trace.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOXBRcmo3WVd"
      },
      "source": [
        "# Running on NVIDIA T4 Tensor Cores, 4GPUS/node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_9nYkcVt3WVe",
        "outputId": "3e8d207d-e6ba-40dd-97e9-dbbbe17e1f7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172.28.0.12\n"
          ]
        }
      ],
      "source": [
        "import socket\n",
        "ip=socket.gethostbyname(socket.gethostname())\n",
        "print(ip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DSoDWRJs3WVe",
        "outputId": "63a28024-f62a-4727-ceb6-8bcfb0d8376e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W0625 17:31:09.925000 19317 torch/distributed/run.py:792] \n",
            "W0625 17:31:09.925000 19317 torch/distributed/run.py:792] *****************************************\n",
            "W0625 17:31:09.925000 19317 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "W0625 17:31:09.925000 19317 torch/distributed/run.py:792] *****************************************\n",
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: batch_size = 64\n",
            "Overriding: gradient_accumulation_steps = 40\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 84, in <module>\n",
            "    init_process_group(backend=backend)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in init_process_group\n",
            "    default_pg, _ = _new_process_group_helper(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1982, in _new_process_group_helper\n",
            "    backend_class = ProcessGroupNCCL(\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: batch_size = 64\n",
            "Overriding: gradient_accumulation_steps = 40\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 84, in <module>\n",
            "    init_process_group(backend=backend)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in init_process_group\n",
            "    default_pg, _ = _new_process_group_helper(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1982, in _new_process_group_helper\n",
            "    backend_class = ProcessGroupNCCL(\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: batch_size = 64\n",
            "Overriding: gradient_accumulation_steps = 40\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 84, in <module>\n",
            "    init_process_group(backend=backend)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in init_process_group\n",
            "    default_pg, _ = _new_process_group_helper(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1982, in _new_process_group_helper\n",
            "    backend_class = ProcessGroupNCCL(\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: batch_size = 64\n",
            "Overriding: gradient_accumulation_steps = 40\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 84, in <module>\n",
            "    init_process_group(backend=backend)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in init_process_group\n",
            "    Overriding config with config/train_shakespeare_char.py:default_pg, _ = _new_process_group_helper(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1982, in _new_process_group_helper\n",
            "    backend_class = ProcessGroupNCCL(\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
            "\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: batch_size = 64\n",
            "Overriding: gradient_accumulation_steps = 40\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 84, in <module>\n",
            "    init_process_group(backend=backend)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in init_process_group\n",
            "    default_pg, _ = _new_process_group_helper(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1982, in _new_process_group_helper\n",
            "    backend_class = ProcessGroupNCCL(\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: batch_size = 64\n",
            "Overriding: gradient_accumulation_steps = 40\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 84, in <module>\n",
            "    init_process_group(backend=backend)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in init_process_group\n",
            "    default_pg, _ = _new_process_group_helper(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1982, in _new_process_group_helper\n",
            "    backend_class = ProcessGroupNCCL(\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: batch_size = 64\n",
            "Overriding: gradient_accumulation_steps = 40\n",
            "Traceback (most recent call last):\n",
            "Overriding config with config/train_shakespeare_char.py:\n",
            "  File \"/content/nanoGPT/train.py\", line 84, in <module>\n",
            "    init_process_group(backend=backend)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in init_process_group\n",
            "    # train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "default_pg, _ = _new_process_group_helper(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1982, in _new_process_group_helper\n",
            "Overriding: batch_size = 64\n",
            "Overriding: gradient_accumulation_steps = 40\n",
            "    backend_class = ProcessGroupNCCL(\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 84, in <module>\n",
            "    init_process_group(backend=backend)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in init_process_group\n",
            "    default_pg, _ = _new_process_group_helper(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1982, in _new_process_group_helper\n",
            "    backend_class = ProcessGroupNCCL(\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: batch_size = 64\n",
            "Overriding: gradient_accumulation_steps = 40\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 84, in <module>\n",
            "    init_process_group(backend=backend)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in init_process_group\n",
            "    default_pg, _ = _new_process_group_helper(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1982, in _new_process_group_helper\n",
            "    backend_class = ProcessGroupNCCL(\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: batch_size = 64\n",
            "Overriding: gradient_accumulation_steps = 40\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 84, in <module>\n",
            "    init_process_group(backend=backend)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in init_process_group\n",
            "    default_pg, _ = _new_process_group_helper(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1982, in _new_process_group_helper\n",
            "    backend_class = ProcessGroupNCCL(\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: batch_size = 64\n",
            "Overriding: gradient_accumulation_steps = 40\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 84, in <module>\n",
            "    init_process_group(backend=backend)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in init_process_group\n",
            "    default_pg, _ = _new_process_group_helper(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1982, in _new_process_group_helper\n",
            "    backend_class = ProcessGroupNCCL(\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: batch_size = 64\n",
            "Overriding: gradient_accumulation_steps = 40\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/train.py\", line 84, in <module>\n",
            "    init_process_group(backend=backend)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in init_process_group\n",
            "    default_pg, _ = _new_process_group_helper(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1982, in _new_process_group_helper\n",
            "    backend_class = ProcessGroupNCCL(\n",
            "                    ^^^^^^^^^^^^^^^^^\n",
            "ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n",
            "W0625 17:31:41.209000 19317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 19332 closing signal SIGTERM\n",
            "W0625 17:31:41.210000 19317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 19333 closing signal SIGTERM\n",
            "W0625 17:31:41.210000 19317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 19335 closing signal SIGTERM\n",
            "W0625 17:31:41.210000 19317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 19336 closing signal SIGTERM\n",
            "W0625 17:31:41.211000 19317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 19337 closing signal SIGTERM\n",
            "W0625 17:31:41.211000 19317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 19338 closing signal SIGTERM\n",
            "W0625 17:31:41.211000 19317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 19339 closing signal SIGTERM\n",
            "W0625 17:31:41.212000 19317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 19340 closing signal SIGTERM\n",
            "W0625 17:31:41.212000 19317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 19341 closing signal SIGTERM\n",
            "W0625 17:31:41.212000 19317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 19342 closing signal SIGTERM\n",
            "W0625 17:31:41.212000 19317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 19343 closing signal SIGTERM\n",
            "E0625 17:31:41.486000 19317 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 19334) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 918, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 909, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "train.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-06-25_17:31:41\n",
            "  host      : 0378f1cd8e1a\n",
            "  rank      : 2 (local_rank: 2)\n",
            "  exitcode  : 1 (pid: 19334)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!export CUDA_VISIBLE_DEVICES=0 #,1,2,3\n",
        "!export MASTER_ADDR=ip\n",
        "!export MASTER_PORT=29500\n",
        "\n",
        "!torchrun \\\n",
        "  --nnodes=1 \\\n",
        "  --node_rank=0 \\\n",
        "  --nproc_per_node=12 \\\n",
        "  --master_addr=$ip \\\n",
        "  --master_port=29500 \\\n",
        "  train.py \\\n",
        "    config/train_shakespeare_char.py \\\n",
        "    --batch_size=64 \\\n",
        "    --gradient_accumulation_steps=40"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6ni2Eso3WVf"
      },
      "source": [
        "![llms](https://github.com/mngom2/DNNMLSS/blob/main/images/llms.gif?raw=1)\n",
        "*Source: [Hannibal046/Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1prewFuz3WVz"
      },
      "source": [
        "![emergent](https://github.com/mngom2/DNNMLSS/blob/main/images/emergent-abilities.gif?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IRlj6_J3WVz"
      },
      "source": [
        "![evolllms](https://github.com/mngom2/DNNMLSS/blob/main/images/evolution.gif?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6FMPR4a3WV0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "env_mlss_dnn",
      "language": "python",
      "name": "env_mlss_dnn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}